# Microservicio de Predicción con Kafka y Docker

Este proyecto implementa un microservicio de predicción utilizando Apache Kafka para la comunicación asíncrona y Docker para la contenerización.

## Estructura del Proyecto

```
Kafka/
├── app/
│   ├── prediction_client.py    # Cliente para enviar solicitudes
│   └── prediction_service.py   # Servicio de predicción
├── docker/
│   ├── Dockerfile             # Configuración de la imagen Docker
│   └── requirements.txt       # Dependencias Python
└── models/                    # Directorio para modelos ML
```

## Componentes Principales

### Servicio de Predicción
- Implementado en `prediction_service.py`
- Consume mensajes del topic `prediction_requests`
- Procesa predicciones usando un modelo ML
- Envía resultados al topic `prediction_results`

### Cliente de Predicción
- Implementado en `prediction_client.py`
- Envía solicitudes de predicción
- Escucha respuestas del servicio

## Configuración y Despliegue

### 1. Crear Red Docker
```bash
docker network create microservices-net
```

### 2. Iniciar Zookeeper
```bash
docker run -d \
    --name zookeeper \
    --network microservices-net \
    -p 2181:2181 \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    -e ZOOKEEPER_TICK_TIME=2000 \
    confluentinc/cp-zookeeper:latest
```

### 3. Iniciar Kafka
```bash
docker run -d \
    --name kafka \
    --network microservices-net \
    -p 29092:29092 \
    -e KAFKA_BROKER_ID=1 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092 \
    -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT \
    -e KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT \
    -e KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:latest
```

### 4. Construir la Imagen del Servicio
```bash
docker build -t microservicio-kafka:1.0 -f docker/Dockerfile .
```

### 5. Iniciar el Servicio de Predicción
```bash
docker run -d \
    --name prediction-service \
    --network microservices-net \
    -v $(pwd)/models:/app/models \
    -e KAFKA_BOOTSTRAP_SERVERS=kafka:9092 \
    microservicio-kafka:1.0
```

### 6. Detener y Limpiar
Para detener y eliminar todos los contenedores:
```bash
docker stop prediction-service kafka zookeeper
docker rm prediction-service kafka zookeeper
docker network rm microservices-net
```

## Puertos y Networking

Los servicios utilizan los siguientes puertos:
- Zookeeper: 2181 (expuesto al host)
- Kafka: 9092 (interno), 29092 (expuesto al host)
- Servicio de Predicción: 8080 (métricas)

La comunicación entre contenedores se realiza a través de la red Docker `microservices-net`.

## Uso del Cliente

1. Asegúrate de tener el modelo ML en el directorio `models/`:
```bash
cp tu_modelo.pkl models/random_forest_model.pkl
```

2. El cliente se ejecuta desde el host, no desde Docker. Usa la configuración:
```python
producer_conf = {
    'bootstrap.servers': 'localhost:29092'  # Puerto expuesto de Kafka
}
```

3. Ejecuta el cliente:
```bash
python app/prediction_client.py
```

## Variables de Entorno

- `KAFKA_BOOTSTRAP_SERVERS`: Dirección del broker Kafka
- `MODEL_PATH`: Ruta al modelo ML
- `REQUEST_TOPIC`: Topic para solicitudes
- `RESPONSE_TOPIC`: Topic para respuestas

## Dependencias Principales

- confluent-kafka: Cliente Kafka
- scikit-learn: Biblioteca ML
- joblib: Carga de modelos
- numpy: Procesamiento numérico
- fastapi: API REST (opcional)
- grpcio: Soporte gRPC (opcional)

## Notas
- El servicio espera un modelo de Random Forest con 13 características de entrada
- Los contenedores se comunican a través de la red `microservices-net`
- El modelo debe montarse como volumen en `/app/models`
